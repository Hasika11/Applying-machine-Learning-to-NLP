{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASSIGNMENT: 2\n",
    "#Task-1: Part Of Speech Tagging\n",
    "#Using GRU with Keras: POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasik\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Data using NLTK Treebank Corpus\n",
    "DATA_DIR = os.getcwd()\n",
    "open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"w\").close()\n",
    "open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"w\").close()\n",
    "\n",
    "fedata = open(os.path.join(DATA_DIR, \"treebank_sents.txt\"), \"a\")\n",
    "ffdata = open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"a\")\n",
    "sents = nltk.corpus.treebank.tagged_sents()\n",
    "for sent in sents:\n",
    "    words, poss = [], []\n",
    "    for word, pos in sent:\n",
    "        if pos == \"-NONE-\":\n",
    "            continue\n",
    "        words.append(word)\n",
    "        poss.append(pos)\n",
    "    fedata.write(\" \". join(words))\n",
    "    fedata.write('\\n')\n",
    "    ffdata.write(\" \".join(poss))\n",
    "    ffdata.write('\\n')\n",
    "fedata.close()\n",
    "ffdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Vocabularies for the words in the sentences for POS Tagging\n",
    "def parse_sentences(filename):\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs, maxlen = 0, 0\n",
    "    fin =open(filename, 'rb')\n",
    "    for line in fin:\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        num_recs += 1\n",
    "    fin.close()\n",
    "    return word_freqs, maxlen, num_recs\n",
    "\n",
    "s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_sents.txt\"))\n",
    "t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_poss.txt\"))\n",
    "# print(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing the each row of the input as a sequence of word indices.\n",
    "MAX_SEQLEN = 250\n",
    "S_MAX_FEATURES = 5000\n",
    "T_MAX_FEATURES = 45\n",
    "s_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2\n",
    "s_word2index = {x[0]:i+2 for i, x in\n",
    "enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\n",
    "s_word2index[\"PAD\"] = 0\n",
    "s_word2index[\"UNK\"] = 1\n",
    "s_index2word = {v:k for k, v in s_word2index.items()}\n",
    "t_vocabsize = len(t_wordfreqs) + 1\n",
    "t_word2index = {x[0]:i for i, x in\n",
    "enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\n",
    "t_word2index[\"PAD\"] = 0\n",
    "t_index2word = {v:k for k, v in t_word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the data sets to feed to the network and converting the input sentences to into a word sequence.\n",
    "def build_tensor(filename, numrecs, word2index, maxlen,make_categorical=False, num_classes=0):\n",
    "    data = np.empty((numrecs, ), dtype=list)\n",
    "    fin = open(filename, \"rb\")\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        wids = []\n",
    "        for word in line.strip().lower().split():\n",
    "            if word in word2index:\n",
    "                wids.append(word2index[word])\n",
    "            else:\n",
    "                wids.append(word2index['UNK'])\n",
    "        if make_categorical:\n",
    "            data[i] = np_utils.to_categorical(wids,num_classes=num_classes)\n",
    "        else:\n",
    "            data[i] = wids\n",
    "        i += 1\n",
    "    fin.close()\n",
    "    pdata = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    return pdata\n",
    "\n",
    "X = build_tensor(os.path.join(DATA_DIR, \"treebank_sents.txt\"),s_numrecs, s_word2index, MAX_SEQLEN)\n",
    "Y = build_tensor(os.path.join(DATA_DIR, \"treebank_poss.txt\"),t_numrecs, t_word2index, MAX_SEQLEN,\n",
    "                 True, t_vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into 80-20 train-test split.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "EMBED_SIZE = 90\n",
    "HIDDEN_SIZE = 60\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "model = Sequential()\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE,input_length=MAX_SEQLEN))\n",
    "#model.add(SpatialDropout1D(Dropout(0.2)))\n",
    "model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(MAX_SEQLEN))\n",
    "model.add(GRU(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/2\n",
      "3131/3131 [==============================] - 58s 19ms/step - loss: 0.3026 - acc: 0.8148 - val_loss: 0.2930 - val_acc: 0.9146\n",
      "Epoch 2/2\n",
      "3131/3131 [==============================] - 54s 17ms/step - loss: 0.2864 - acc: 0.9071 - val_loss: 0.2918 - val_acc: 0.9109\n",
      "783/783 [==============================] - 4s 5ms/step\n",
      "Test score: 0.292, accuracy: 0.911\n"
     ]
    }
   ],
   "source": [
    "#Training the model \n",
    "model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "validation_data=[Xtest, Ytest])\n",
    "score, acc = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
